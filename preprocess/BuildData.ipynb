{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from transformers import BertTokenizerFast\n",
    "import copy\n",
    "import torch\n",
    "from common.utils import Preprocessor\n",
    "import yaml\n",
    "import logging\n",
    "from pprint import pprint\n",
    "from IPython.core.debugger import set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from yaml import CLoader as Loader, CDumper as Dumper\n",
    "except ImportError:\n",
    "    from yaml import Loader, Dumper\n",
    "config = yaml.load(open(\"build_data_config.yaml\", \"r\"), Loader = yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = config[\"exp_name\"]\n",
    "data_in_dir = os.path.join(config[\"data_in_dir\"], exp_name)\n",
    "data_out_dir = os.path.join(config[\"data_out_dir\"], exp_name)\n",
    "if not os.path.exists(data_out_dir):\n",
    "    os.makedirs(data_out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name2data = {}\n",
    "for path, folds, files in os.walk(data_in_dir):\n",
    "    for file_name in files:\n",
    "        file_path = os.path.join(path, file_name)\n",
    "        file_name = re.match(\"(.*?)\\.json\", file_name).group(1)\n",
    "        file_name2data[file_name] = json.load(open(file_path, \"r\", encoding = \"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['huayandan_val_0620', 'huayandan_test_0620', 'huayandan_train_0620'])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_name2data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../pretrained_models/BERT_EMR\n"
     ]
    }
   ],
   "source": [
    "# @specific\n",
    "if config[\"encoder\"] == \"BERT\":\n",
    "    print(config['bert_path'])\n",
    "    tokenizer = BertTokenizerFast.from_pretrained(config[\"bert_path\"], add_special_tokens = False, do_lower_case = False)\n",
    "    tokenize = tokenizer.tokenize\n",
    "    get_tok2char_span_map = lambda text: tokenizer.encode_plus(text, return_offsets_mapping = True, add_special_tokens = False)[\"offset_mapping\"]\n",
    "elif config[\"encoder\"] == \"BiLSTM\":\n",
    "    tokenize = lambda text: text.split(\" \")\n",
    "    def get_tok2char_span_map(text):\n",
    "        tokens = tokenize(text)\n",
    "        tok2char_span = []\n",
    "        char_num = 0\n",
    "        for tok in tokens:\n",
    "            tok2char_span.append((char_num, char_num + len(tok)))\n",
    "            char_num += len(tok) + 1 # +1: whitespace\n",
    "        return tok2char_span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = Preprocessor(tokenize_func = tokenize, \n",
    "                            get_tok2char_span_map_func = get_tok2char_span_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_format = config[\"ori_data_format\"]\n",
    "if ori_format != \"tplinker\": # if tplinker, skip transforming\n",
    "    for file_name, data in file_name2data.items():\n",
    "        if \"train\" in file_name:\n",
    "            data_type = \"train\"\n",
    "        if \"valid\" in file_name:\n",
    "            data_type = \"valid\"\n",
    "        if \"test\" in file_name:\n",
    "            data_type = \"test\"\n",
    "        data = preprocessor.transform_data(data, ori_format = ori_format, dataset_type = data_type, add_id = True)\n",
    "        file_name2data[file_name] = data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Clean and Add Spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check token level span\n",
    "def check_tok_span(data):\n",
    "    def extr_ent(text, tok_span, tok2char_span):\n",
    "        char_span_list = tok2char_span[tok_span[0]:tok_span[1]]\n",
    "        char_span = (char_span_list[0][0], char_span_list[-1][1])\n",
    "        decoded_ent = text[char_span[0]:char_span[1]]\n",
    "        return decoded_ent\n",
    "\n",
    "    span_error_memory = set()\n",
    "    for sample in tqdm(data, desc = \"check tok spans\"):\n",
    "        text = sample[\"text\"]\n",
    "        tok2char_span = get_tok2char_span_map(text)\n",
    "        for ent in sample[\"entity_list\"]:\n",
    "            tok_span = ent[\"tok_span\"]\n",
    "            if extr_ent(text, tok_span, tok2char_span) != ent[\"text\"]:\n",
    "                span_error_memory.add(\"extr ent: {}---gold ent: {}\".format(extr_ent(text, tok_span, tok2char_span), ent[\"text\"]))\n",
    "                \n",
    "        for rel in sample[\"relation_list\"]:\n",
    "            subj_tok_span, obj_tok_span = rel[\"subj_tok_span\"], rel[\"obj_tok_span\"]\n",
    "            if extr_ent(text, subj_tok_span, tok2char_span) != rel[\"subject\"]:\n",
    "                span_error_memory.add(\"extr: {}---gold: {}\".format(extr_ent(text, subj_tok_span, tok2char_span), rel[\"subject\"]))\n",
    "            if extr_ent(text, obj_tok_span, tok2char_span) != rel[\"object\"]:\n",
    "                span_error_memory.add(\"extr: {}---gold: {}\".format(extr_ent(text, obj_tok_span, tok2char_span), rel[\"object\"]))\n",
    "                \n",
    "    return span_error_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "clean data: 100%|██████████| 69/69 [00:00<00:00, 5080.26it/s]\n",
      "adding char level spans: 100%|██████████| 69/69 [00:00<00:00, 358.75it/s]\n",
      "building relation type set and entity type set: 100%|██████████| 69/69 [00:00<00:00, 16069.24it/s]\n",
      "adding token level spans: 100%|██████████| 69/69 [00:00<00:00, 760.81it/s]\n",
      "check tok spans: 100%|██████████| 69/69 [00:00<00:00, 763.94it/s]\n",
      "clean data: 100%|██████████| 55/55 [00:00<00:00, 4029.88it/s]\n",
      "adding char level spans:  35%|███▍      | 19/55 [00:00<00:00, 185.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'extr: IU/L---gold: U/L', 'extr ent: ALT---gold ent: A', 'extr: ALB---gold: L', 'extr ent: LMID---gold ent: L', 'extr ent: mIU/mL---gold ent: IU/mL', 'extr: TP---gold: P', 'extr: LYM---gold: L', 'extr ent: ALP---gold ent: P', 'extr ent: TP---gold ent: P', 'extr ent: HONH---gold ent: HON', 'extr: min---gold: i', 'extr ent: TBIL---gold ent: L', 'extr: fLe---gold: fL', 'extr ent: CKMB---gold ent: CK', 'extr ent: DBIL---gold ent: L', 'extr: APTT---gold: PT', 'extr ent: ALT---gold ent: L', 'extr: SpO2---gold: S', 'extr: AST---gold: S', 'extr ent: mAST---gold ent: AST', 'extr: GA2---gold: GA', 'extr ent: 糖化血红蛋白Ala---gold ent: 糖化血红蛋白A', 'extr ent: IDBIL---gold ent: L', 'extr: EPi---gold: i', 'extr ent: SpO2---gold ent: S', 'extr ent: EPi---gold ent: P', 'extr ent: IU/L---gold ent: U/L', 'extr: mmol---gold: mmo', 'extr ent: PCT---gold ent: P', 'extr ent: mS/CO---gold ent: S/CO', 'extr: PCT---gold: P', 'extr ent: EPi---gold ent: i', 'extr ent: NEU---gold ent: N', 'extr: fLf---gold: fL', 'extr ent: IDBIL---gold ent: DBIL', 'extr ent: fLf---gold ent: fL', 'extr: L0.0---gold: 0.0', 'extr: mIU/mL---gold: IU/mL', 'extr: mAST---gold: AST', 'extr ent: mumol/L---gold ent: umol/L', 'extr: HONH---gold: HON', 'extr: TBIL---gold: L', 'extr: 糖化血红蛋白Ala---gold: 糖化血红蛋白A', 'extr: mumol/L---gold: umol/L', 'extr ent: PLT---gold ent: L', 'extr ent: min---gold ent: i', 'extr ent: mg/l---gold ent: g/l', 'extr: mmmol/L---gold: mmol/L', 'extr: ALT---gold: A', 'extr ent: LYM---gold ent: L', 'extr: NEU---gold: N', 'extr: CP---gold: P', 'extr ent: CP---gold ent: P', 'extr ent: 暗酸性粒细胞百分L0---gold ent: 暗酸性粒细胞百分L', 'extr ent: mg/L---gold ent: g/L', 'extr ent: L0.0---gold ent: 0.0', 'extr: ALT---gold: L', 'extr: PLT---gold: L', 'extr ent: mmol---gold ent: mmo', 'extr: EPi---gold: P', 'extr ent: ALB---gold ent: L', 'extr ent: 10∧9---gold ent: 9', 'extr: IDBIL---gold: L', 'extr ent: AST---gold ent: A', 'extr: AST---gold: A', 'extr: PK---gold: K', 'extr: mg/L---gold: g/L', 'extr ent: GA2---gold ent: GA', 'extr ent: PK---gold ent: K', 'extr: mg/l---gold: g/l', 'extr: mS/CO---gold: S/CO', 'extr: LMID---gold: L', 'extr: CKMB---gold: CK', 'extr: 暗酸性粒细胞百分L0---gold: 暗酸性粒细胞百分L', 'extr ent: CRP---gold ent: P', 'extr ent: mmmol/L---gold ent: mmol/L', 'extr ent: fLe---gold ent: fL', 'extr: 10∧9---gold: 9', 'extr ent: APTT---gold ent: PT', 'extr ent: AST---gold ent: S', 'extr: ALP---gold: P', 'extr: CRP---gold: P', 'extr: IDBIL---gold: DBIL', 'extr: DBIL---gold: L'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "adding char level spans: 100%|██████████| 55/55 [00:00<00:00, 231.70it/s]\n",
      "building relation type set and entity type set: 100%|██████████| 55/55 [00:00<00:00, 14873.42it/s]\n",
      "adding token level spans: 100%|██████████| 55/55 [00:00<00:00, 397.96it/s]\n",
      "check tok spans: 100%|██████████| 55/55 [00:00<00:00, 679.75it/s]\n",
      "clean data: 100%|██████████| 273/273 [00:00<00:00, 7547.09it/s]\n",
      "adding char level spans:   8%|▊         | 21/273 [00:00<00:03, 67.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'extr ent: HGB★---gold ent: HGB', 'extr: IU/L---gold: U/L', 'extr: x10 9/L---gold: 10 9/L', 'extr ent: 26-140U---gold ent: 26-140', 'extr: =2o1/153---gold: =2o1/1', 'extr ent: 2001U/ml---gold ent: U/ml', 'extr: MCHC---gold: MCH', 'extr ent: 6g/L---gold ent: g/L', 'extr ent: 0.6-1.1g---gold ent: 0.6-1.1', 'extr ent: TP---gold ent: T', 'extr: L120 110---gold: 120 110', 'extr ent: 4mmol/L---gold ent: mmol/L', 'extr: 40IU/L---gold: IU/L', 'extr: 2mmol/L---gold: mmol/L', 'extr ent: MCHC---gold ent: MCH', 'extr: 3.5-5.2mm---gold: 3.5-5.2', 'extr ent: 17D---gold ent: 17', 'extr ent: /15---gold ent: /1', 'extr: TP---gold: P', 'extr ent: 201U/ml---gold ent: U/ml', 'extr: 7-32U---gold: 7-32', 'extr ent: ALP---gold ent: P', 'extr ent: L4.3--5.8---gold ent: 4.3--5.8', 'extr ent: 1mmol/L---gold ent: mmol/L', 'extr: W11---gold: 11', 'extr ent: 40IU/L---gold ent: U/L', 'extr ent: 245IU/L---gold ent: U/L', 'extr: ChT---gold: T', 'extr ent: 501U/L---gold ent: U/L', 'extr ent: TP---gold ent: P', 'extr: /153---gold: /1', 'extr: min---gold: i', 'extr: fL---gold: L', 'extr ent: 1741U/L---gold ent: U/L', 'extr: 125U/L---gold: U/L', 'extr ent: HGB★血红蛋白---gold ent: ★血红蛋白', 'extr ent: CKMB---gold ent: CK', 'extr ent: W11---gold ent: 11', 'extr: TU/L---gold: U/L', 'extr: <5.2mm---gold: <5.2', 'extr: 109-245U---gold: 109-245', 'extr ent: x10^9/1---gold ent: 10^9/1', 'extr: TP---gold: T', 'extr: 1mmol/L---gold: mmol/L', 'extr: GA2---gold: GA', 'extr ent: 10-60U---gold ent: 10-60', 'extr ent: LCR---gold ent: L', 'extr ent: 42-141U---gold ent: 42-141', 'extr ent: 220U/L---gold ent: U/L', 'extr: 0-25U---gold: 0-25', 'extr ent: GGT---gold ent: G', 'extr ent: <2001U---gold ent: <2001', 'extr ent: /153---gold ent: /1', 'extr ent: CK---gold ent: K', 'extr ent: lL---gold ent: L', 'extr ent: 2.68-8.2mm---gold ent: 2.68-8.2', 'extr ent: 109-245IU---gold ent: 109-245', 'extr ent: 25U/L---gold ent: U/L', 'extr ent: 0.6-1.1m---gold ent: 0.6-1.1', 'extr: EPi---gold: i', 'extr ent: 1-1.6g---gold ent: 1-1.6', 'extr: 201U/ml---gold: U/ml', 'extr ent: 32U/L---gold ent: U/L', 'extr ent: 2mmo1/L---gold ent: mmo1/L', 'extr: x109/1---gold: 109/1', 'extr ent: EPi---gold ent: P', 'extr ent: IU/L---gold ent: U/L', 'extr: <2001U---gold: <2001', 'extr ent: <220U---gold ent: <220', 'extr ent: PCT---gold ent: P', 'extr: 141U/L---gold: U/L', 'extr: LCR---gold: L', 'extr: 2.68-8.2mm---gold: 2.68-8.2', 'extr: PCT---gold: P', 'extr: IDBIL---gold: DBIL', 'extr ent: DEPT---gold ent: P', 'extr ent: EPi---gold ent: i', 'extr ent: x10^9/L---gold ent: 10^9/L', 'extr: 23C反应蛋白---gold: C反应蛋白', 'extr: <220U---gold: <220', 'extr ent: 23C反应蛋白---gold ent: C反应蛋白', 'extr ent: <5.2mm---gold ent: <5.2', 'extr: 220U/L---gold: U/L', 'extr ent: fL---gold ent: L', 'extr: HCT★红细胞压积---gold: ★红细胞压积', 'extr ent: IDBIL---gold ent: DBIL', 'extr: 245U/L---gold: U/L', 'extr ent: <10---gold ent: <1', 'extr ent: ChT---gold ent: T', 'extr ent: =2o1/153---gold ent: =2o1/1', 'extr: GGT---gold: G', 'extr: <201U---gold: <201', 'extr: 2.9-8.2mm---gold: 2.9-8.2', 'extr: x10^9/L---gold: 10^9/L', 'extr ent: U7---gold ent: 7', 'extr ent: RBC★红细胞总数---gold ent: ★红细胞总数', 'extr ent: 245IU/L---gold ent: IU/L', 'extr ent: 3.5-5.2mm---gold ent: 3.5-5.2', 'extr: 182U/L---gold: U/L', 'extr: ↑fL---gold: fL', 'extr: cGFR(基于简化MDRL82---gold: cGFR(基于简化MDRL', 'extr ent: 245U/L---gold ent: U/L', 'extr: <14---gold: <1', 'extr: 2, 6-3.4m---gold: 2, 6-3.4', 'extr ent: 125U/L---gold ent: U/L', 'extr ent: HCT★---gold ent: HCT', 'extr: 2mmo1/L---gold: mmo1/L', 'extr ent: <201U---gold ent: <201', 'extr: WBC★白细胞总数---gold: ★白细胞总数', 'extr ent: mumol/L---gold ent: umol/L', 'extr: ALT---gold: T', 'extr: NRBC---gold: RBC', 'extr: f1---gold: 1', 'extr: 1g/L---gold: g/L', 'extr ent: 40U/L---gold ent: U/L', 'extr: WBC★---gold: WBC', 'extr ent: 140U/L---gold ent: U/L', 'extr ent: 320m0SM/L---gold ent: m0SM/L', 'extr ent: 109-245U---gold ent: 109-245', 'extr ent: x10 9/L---gold ent: 10 9/L', 'extr: PLT★---gold: PLT', 'extr: HGB★---gold: HGB', 'extr: 40U/L---gold: U/L', 'extr: PLT★血小板总数---gold: ★血小板总数', 'extr: GLU---gold: G', 'extr ent: 141U/L---gold ent: U/L', 'extr ent: PLT★---gold ent: PLT', 'extr ent: 182U/L---gold ent: U/L', 'extr: mumol/L---gold: umol/L', 'extr ent: 1L---gold ent: 1', 'extr: 0.4-2.3mm---gold: 0.4-2.3', 'extr ent: WBC★---gold ent: WBC', 'extr ent: min---gold ent: i', 'extr: 245IU/L---gold: U/L', 'extr: 140U/L---gold: U/L', 'extr: L4.3--5.8---gold: 4.3--5.8', 'extr ent: HCT★红细胞压积---gold ent: ★红细胞压积', 'extr: IMP---gold: P', 'extr: 60U/L---gold: U/L', 'extr ent: MDRL82.5---gold ent: 82.5', 'extr ent: 2mmol/L---gold ent: mmol/L', 'extr ent: REPORT---gold ent: P', 'extr ent: LYMPH---gold ent: L', 'extr ent: PLT★血小板总数---gold ent: ★血小板总数', 'extr: 501U/L---gold: U/L', 'extr ent: Th1L---gold ent: T', 'extr ent: L120 110---gold ent: 120 110', 'extr: CP---gold: P', 'extr ent: 2.9-8.2mm---gold ent: 2.9-8.2', 'extr ent: CP---gold ent: P', 'extr: HGB★血红蛋白---gold: ★血红蛋白', 'extr: 4mmol/L---gold: mmol/L', 'extr: 32U/L---gold: U/L', 'extr ent: 2, 6-3.4m---gold ent: 2, 6-3.4', 'extr ent: mg/L---gold ent: g/L', 'extr: 26-140U---gold: 26-140', 'extr ent: 3mmol/L---gold ent: mmol/L', 'extr: SAMPLING---gold: P', 'extr ent: 7-32U---gold ent: 7-32', 'extr ent: 1g/L---gold ent: g/L', 'extr: 0.6-1.1g---gold: 0.6-1.1', 'extr: DEPT---gold: P', 'extr: EPi---gold: P', 'extr ent: NRBC---gold ent: RBC', 'extr ent: x10~9/L---gold ent: 10~9/L', 'extr: Th1L---gold: T', 'extr ent: f1---gold ent: 1', 'extr ent: TU/L---gold ent: U/L', 'extr: 45-125U---gold: 45-125', 'extr ent: 0-25U---gold ent: 0-25', 'extr ent: 60U/L---gold ent: U/L', 'extr ent: SAMPLING---gold ent: P', 'extr: CK---gold: K', 'extr: RBC★---gold: RBC', 'extr: 25U/L---gold: U/L', 'extr: /15---gold: /1', 'extr ent: ↓fL---gold ent: fL', 'extr ent: 280-320m0SM---gold ent: 280-320', 'extr: 1741U/L---gold: U/L', 'extr: MDRL82.5---gold: 82.5', 'extr ent: ↑fL---gold ent: fL', 'extr ent: PLT★---gold ent: L', 'extr: 320m0SM/L---gold: m0SM/L', 'extr ent: x109/1---gold ent: 109/1', 'extr: 17D---gold: 17', 'extr ent: 40IU/L---gold ent: IU/L', 'extr: mg/L---gold: g/L', 'extr ent: GA2---gold ent: GA', 'extr: PLT★---gold: L', 'extr: 72-182U---gold: 72-182', 'extr ent: WBC★白细胞总数---gold ent: ★白细胞总数', 'extr: 0.6-1.1m---gold: 0.6-1.1', 'extr: 245IU/L---gold: IU/L', 'extr ent: 45-125U---gold ent: 45-125', 'extr: 3mmol/L---gold: mmol/L', 'extr: 2001U/ml---gold: U/ml', 'extr: LYMPH---gold: L', 'extr: 40IU/L---gold: U/L', 'extr: REPORT---gold: P', 'extr: 15-40IU---gold: 15-40', 'extr: RBC★红细胞总数---gold: ★红细胞总数', 'extr ent: <14---gold ent: <1', 'extr: CKMB---gold: CK', 'extr ent: IMP---gold ent: P', 'extr: PA---gold: P', 'extr: <10---gold: <1', 'extr ent: PA---gold ent: P', 'extr ent: RBC★---gold ent: RBC', 'extr: HCT★---gold: HCT', 'extr: U7---gold: 7', 'extr: lL---gold: L', 'extr ent: CRP---gold ent: P', 'extr ent: GLU---gold ent: G', 'extr ent: 0-40U---gold ent: 0-40', 'extr ent: ALT---gold ent: T', 'extr: 1-1.6g---gold: 1-1.6', 'extr: 42-141U---gold: 42-141', 'extr: x10^9/1---gold: 10^9/1', 'extr ent: 0.4-2.3mm---gold ent: 0.4-2.3', 'extr: 10-60U---gold: 10-60', 'extr ent: cGFR(基于简化MDRL82---gold ent: cGFR(基于简化MDRL', 'extr: x1012/1---gold: 1012/1', 'extr ent: 72-182U---gold ent: 72-182', 'extr ent: x1012/1---gold ent: 1012/1', 'extr: ALP---gold: P', 'extr: ↓fL---gold: fL', 'extr: 6g/L---gold: g/L', 'extr: x10~9/L---gold: 10~9/L', 'extr ent: 15-40IU---gold ent: 15-40', 'extr: CRP---gold: P', 'extr: 109-245IU---gold: 109-245', 'extr: 0-40U---gold: 0-40', 'extr: 280-320m0SM---gold: 280-320'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "adding char level spans: 100%|██████████| 273/273 [00:00<00:00, 275.28it/s]\n",
      "building relation type set and entity type set: 100%|██████████| 273/273 [00:00<00:00, 9955.18it/s]\n",
      "adding token level spans: 100%|██████████| 273/273 [00:00<00:00, 369.23it/s]\n",
      "check tok spans: 100%|██████████| 273/273 [00:00<00:00, 698.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'extr: APTT---gold: TT', 'extr: IU/L---gold: U/L', 'extr ent: 1W1---gold ent: 1', 'extr ent: ALT---gold ent: A', 'extr ent: GLO---gold ent: L', 'extr: TRAB---gold: T', 'extr: pC02---gold: C', 'extr ent: 5-40U---gold ent: 5-40', 'extr ent: eL---gold ent: L', 'extr: 1L---gold: 1', 'extr ent: ag/L---gold ent: g/L', 'extr: ALB---gold: L', 'extr: MCHC---gold: MCH', 'extr: 31Apok/ApeB---gold: Apok/ApeB', 'extr ent: LMID---gold ent: L', 'extr ent: mIU/mL---gold ent: IU/mL', 'extr ent: NAPTT---gold ent: APTT', 'extr: 138U/L---gold: U/L', 'extr: ALP---gold: L', 'extr: 2mmol/L---gold: mmol/L', 'extr ent: 4-20U---gold ent: 4-20', 'extr ent: MCHC---gold ent: MCH', 'extr: APTTR---gold: TT', 'extr: TP---gold: P', 'extr: LYM---gold: L', 'extr ent: 2LP---gold ent: P', 'extr ent: ALP---gold ent: P', 'extr ent: DL---gold ent: L', 'extr: GLO---gold: L', 'extr ent: APOuL1---gold ent: P', 'extr ent: LiP---gold ent: P', 'extr ent: NAPTT---gold ent: PT', 'extr: Ca---gold: C', 'extr: 120-230U---gold: 120-230', 'extr ent: TP---gold ent: P', 'extr ent: M4C3---gold ent: C3', 'extr ent: APTT---gold ent: TT', 'extr ent: 15-40U---gold ent: 15-40', 'extr: 7A01---gold: 01', 'extr ent: 20U/L---gold ent: U/L', 'extr: 3700-13200U---gold: 3700-13200', 'extr: 10ADA---gold: ADA', 'extr ent: NAPTT---gold ent: TT', 'extr: DL---gold: L', 'extr ent: KU/L---gold ent: U/L', 'extr ent: 10ADA---gold ent: ADA', 'extr: TBA---gold: TB', 'extr: mmol/L---gold: mol/L', 'extr: Diner---gold: Di', 'extr ent: 3700-13200U---gold ent: 3700-13200', 'extr: 3g/1---gold: g/1', 'extr ent: TBIL---gold ent: L', 'extr ent: 7A01---gold ent: 01', 'extr: KU/1---gold: U/1', 'extr ent: TBA---gold ent: TB', 'extr ent: APTTR---gold ent: TT', 'extr ent: DBIL---gold ent: L', 'extr ent: SBC---gold ent: C', 'extr: APTT---gold: PT', 'extr: TC02---gold: C', 'extr: 11TBA---gold: TBA', 'extr: 紅细国分布宽度-SD39---gold: 紅细国分布宽度-SD', 'extr ent: APTTR---gold ent: PT', 'extr: TG---gold: T', 'extr ent: ALT---gold ent: L', 'extr: TC02---gold: T', 'extr ent: 25LPPC---gold ent: LPP', 'extr: TPO---gold: T', 'extr: AST---gold: S', 'extr ent: LDL---gold ent: L', 'extr: 2LP---gold: L', 'extr ent: hsCRP---gold ent: CRP', 'extr: PTA---gold: PT', 'extr ent: ABC---gold ent: C', 'extr ent: 1U---gold ent: 1', 'extr: 25LPPC反应金白---gold: C反应金白', 'extr ent: UF---gold ent: F', 'extr: 1W=1---gold: W=1', 'extr: 0mm---gold: 0', 'extr: T3---gold: T', 'extr: GLD---gold: L', 'extr: D0-1---gold: 0-1', 'extr: ALB---gold: A', 'extr: MONOP---gold: MO', 'extr ent: 13200U/L---gold ent: U/L', 'extr ent: UAL---gold ent: U', 'extr: 35-138U---gold: 35-138', 'extr ent: T-Bili---gold ent: T-Bi', 'extr: APOuL1---gold: P', 'extr ent: 4T---gold ent: 4', 'extr ent: IU/L---gold ent: U/L', 'extr ent: TC02---gold ent: C', 'extr: SBC---gold: C', 'extr ent: 1W=1---gold ent: W=1', 'extr: ALP---gold: A', 'extr: 15-50U---gold: 15-50', 'extr ent: mS/CO---gold ent: S/CO', 'extr ent: 11TBA---gold ent: TBA', 'extr: va3W00Vn0---gold: 0', 'extr: ABC---gold: C', 'extr ent: mmol/L---gold ent: mol/L', 'extr ent: FT3---gold ent: T', 'extr ent: 25LPPC反应金白---gold ent: C反应金白', 'extr: CFEA---gold: F', 'extr ent: APTT正常对照---gold ent: TT正常对照', 'extr: UF---gold: F', 'extr: FT4---gold: T', 'extr ent: WL---gold ent: L', 'extr ent: TB1---gold ent: T', 'extr: CHDL---gold: HDL', 'extr ent: 120-230U---gold ent: 120-230', 'extr: 13200U/L---gold: U/L', 'extr: SD39.3---gold: 39.3', 'extr: UAL---gold: U', 'extr ent: ALP---gold ent: L', 'extr ent: IDBIL---gold ent: DBIL', 'extr ent: A5T---gold ent: 5', 'extr ent: <10---gold ent: <1', 'extr ent: HDL---gold ent: DL', 'extr: 34U/L---gold: U/L', 'extr ent: pC02---gold ent: C', 'extr ent: BAS---gold ent: AS', 'extr: APOuL1---gold: L', 'extr: TBA---gold: A', 'extr: 230U/L---gold: U/L', 'extr: PLTPLT---gold: PLT', 'extr ent: PTA---gold ent: PT', 'extr ent: ALB---gold ent: A', 'extr ent: IBILI---gold ent: L', 'extr ent: NPT---gold ent: PT', 'extr ent: t1---gold ent: 1', 'extr ent: COHb---gold ent: C', 'extr ent: TPO---gold ent: T', 'extr: eL---gold: L', 'extr: CHDL---gold: DL', 'extr: mIU/mL---gold: IU/mL', 'extr ent: ALP---gold ent: AL', 'extr: DBIL---gold: L', 'extr: CHDL---gold: L', 'extr: ALP---gold: AL', 'extr ent: 90.3g---gold ent: 90.3', 'extr: C1---gold: C', 'extr: Eg/L---gold: g/L', 'extr ent: TG---gold ent: T', 'extr: T-Bili---gold: T-Bi', 'extr: KU/L---gold: U/L', 'extr: BAS---gold: AS', 'extr ent: 40U/L---gold ent: U/L', 'extr ent: HDL---gold ent: L', 'extr ent: TSH---gold ent: T', 'extr: 40U/L---gold: U/L', 'extr ent: 138U/L---gold ent: U/L', 'extr: COHb---gold: C', 'extr: TBIL---gold: L', 'extr: 12AST/Al---gold: AST/Al', 'extr ent: 0mm---gold ent: 0', 'extr ent: TBILI---gold ent: L', 'extr ent: 1L---gold ent: 1', 'extr ent: MONOP---gold ent: MO', 'extr: hsCRP---gold: CRP', 'extr ent: PLT---gold ent: L', 'extr: Glu---gold: G', 'extr ent: mg/l---gold ent: g/l', 'extr ent: KU/1---gold ent: U/1', 'extr ent: 12AST/Al---gold ent: AST/Al', 'extr ent: FT3---gold ent: T3', 'extr: GGTP---gold: TP', 'extr: ALT---gold: A', 'extr ent: LH---gold ent: H', 'extr ent: LYM---gold ent: L', 'extr ent: 50U/L---gold ent: U/L', 'extr: HDL---gold: L', 'extr: 5-40U---gold: 5-40', 'extr ent: 2mmol/L---gold ent: mmol/L', 'extr: 50U/L---gold: U/L', 'extr: TG---gold: G', 'extr ent: TRAB---gold ent: T', 'extr: 25LPPC---gold: LPP', 'extr ent: Ca---gold ent: a', 'extr ent: Diner---gold ent: Di', 'extr ent: Glu---gold ent: G', 'extr: CP---gold: P', 'extr: NTT---gold: TT', 'extr: LDH1---gold: LDH', 'extr ent: CP---gold ent: P', 'extr ent: LDL---gold ent: DL', 'extr ent: FT4---gold ent: T', 'extr: ulU/ml---gold: lU/ml', 'extr ent: mg/L---gold ent: g/L', 'extr ent: CHDL---gold ent: HDL', 'extr: ALT---gold: L', 'extr ent: va3W00Vn0---gold ent: 0', 'extr: PLT---gold: L', 'extr ent: T3---gold ent: T', 'extr ent: 3g/1---gold ent: g/1', 'extr ent: C1---gold ent: C', 'extr ent: CHDL---gold ent: DL', 'extr ent: PLTPLT---gold ent: PLT', 'extr: M4C3---gold: C3', 'extr ent: 230U/L---gold ent: U/L', 'extr ent: 12AST---gold ent: AST', 'extr ent: ALB---gold ent: L', 'extr ent: APOuL1---gold ent: L', 'extr: APTT正常对照---gold: TT正常对照', 'extr ent: NTT---gold ent: TT', 'extr ent: Eg/L---gold ent: g/L', 'extr: IBILI---gold: L', 'extr ent: TG---gold ent: G', 'extr ent: ulU/ml---gold ent: lU/ml', 'extr: HDL---gold: DL', 'extr ent: 35-138U---gold ent: 35-138', 'extr: t1---gold: 1', 'extr ent: AST---gold ent: A', 'extr: FT3---gold: T3', 'extr: LDL---gold: L', 'extr: 4-20U---gold: 4-20', 'extr: WL---gold: L', 'extr ent: TBA---gold ent: A', 'extr ent: 2LP---gold ent: L', 'extr: FT3---gold: T', 'extr: AST---gold: A', 'extr: APTTR---gold: PT', 'extr: TB1---gold: T', 'extr: LH---gold: H', 'extr: mg/L---gold: g/L', 'extr ent: TC02---gold ent: T', 'extr ent: 15-50U---gold ent: 15-50', 'extr: NAPTT---gold: TT', 'extr ent: SD39.3---gold ent: 39.3', 'extr: mg/l---gold: g/l', 'extr: 12AST---gold: AST', 'extr: APTTR---gold: APTT', 'extr ent: GGTP---gold ent: TP', 'extr: mS/CO---gold: S/CO', 'extr: 15-40U---gold: 15-40', 'extr: TSH---gold: T', 'extr: Ca---gold: a', 'extr ent: ALP---gold ent: A', 'extr: 4T---gold: 4', 'extr: LMID---gold: L', 'extr: 2LP---gold: P', 'extr: PA---gold: P', 'extr: <10---gold: <1', 'extr ent: PA---gold ent: P', 'extr: NAPTT---gold: PT', 'extr ent: 31Apok/ApeB---gold ent: Apok/ApeB', 'extr ent: H8---gold ent: H', 'extr: A5T---gold: 5', 'extr: 90.3g---gold: 90.3', 'extr ent: 紅细国分布宽度-SD39---gold ent: 紅细国分布宽度-SD', 'extr: ag/L---gold: g/L', 'extr: DBILI---gold: L', 'extr: NAPTT---gold: APTT', 'extr: 20U/L---gold: U/L', 'extr ent: D0-1---gold ent: 0-1', 'extr ent: CHDL---gold ent: L', 'extr ent: DBILI---gold ent: L', 'extr ent: LDH1---gold ent: LDH', 'extr: NPT---gold: PT', 'extr ent: CFEA---gold ent: F', 'extr: LiP---gold: P', 'extr ent: APTT---gold ent: PT', 'extr: TBILI---gold: L', 'extr ent: AST---gold ent: S', 'extr: ALP---gold: P', 'extr ent: APTTR---gold ent: APTT', 'extr ent: Ca---gold ent: C', 'extr ent: VL---gold ent: L', 'extr: LDL---gold: DL', 'extr: 1U/---gold: U/', 'extr: H8---gold: H', 'extr: IDBIL---gold: DBIL', 'extr ent: 34U/L---gold ent: U/L', 'extr: VL---gold: L', 'extr ent: 1U/---gold ent: U/', 'extr ent: GLD---gold ent: L'}\n",
      "{'huayandan_test_0620': {'miss_samples': 0, 'tok_span_error': 233},\n",
      " 'huayandan_train_0620': {'miss_samples': 0, 'tok_span_error': 276},\n",
      " 'huayandan_val_0620': {'miss_samples': 0, 'tok_span_error': 84}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# clean, add char span, tok span\n",
    "# collect relations\n",
    "# check tok spans\n",
    "rel_set = set()\n",
    "ent_set = set()\n",
    "error_statistics = {}\n",
    "for file_name, data in file_name2data.items():\n",
    "    assert len(data) > 0\n",
    "    if \"relation_list\" in data[0]: # train or valid data\n",
    "        # rm redundant whitespaces\n",
    "        # separate by whitespaces\n",
    "        data = preprocessor.clean_data_wo_span(data, separate = config[\"separate_char_by_white\"])\n",
    "        error_statistics[file_name] = {}\n",
    "#         if file_name != \"train_data\":\n",
    "#             set_trace()\n",
    "        # add char span\n",
    "        if config[\"add_char_span\"]:\n",
    "            data, miss_sample_list = preprocessor.add_char_span(data, config[\"ignore_subword\"])\n",
    "            error_statistics[file_name][\"miss_samples\"] = len(miss_sample_list)\n",
    "            \n",
    "#         # clean\n",
    "#         data, bad_samples_w_char_span_error = preprocessor.clean_data_w_span(data)\n",
    "#         error_statistics[file_name][\"char_span_error\"] = len(bad_samples_w_char_span_error)\n",
    "                            \n",
    "        # collect relation types and entity types\n",
    "        for sample in tqdm(data, desc = \"building relation type set and entity type set\"):\n",
    "            if \"entity_list\" not in sample: # if \"entity_list\" not in sample, generate entity list with default type\n",
    "                ent_list = []\n",
    "                for rel in sample[\"relation_list\"]:\n",
    "                    ent_list.append({\n",
    "                        \"text\": rel[\"subject\"],\n",
    "                        \"type\": \"DEFAULT\",\n",
    "                        \"char_span\": rel[\"subj_char_span\"],\n",
    "                    })\n",
    "                    ent_list.append({\n",
    "                        \"text\": rel[\"object\"],\n",
    "                        \"type\": \"DEFAULT\",\n",
    "                        \"char_span\": rel[\"obj_char_span\"],\n",
    "                    })\n",
    "                sample[\"entity_list\"] = ent_list\n",
    "            \n",
    "            for ent in sample[\"entity_list\"]:\n",
    "                ent_set.add(ent[\"type\"])\n",
    "                \n",
    "            for rel in sample[\"relation_list\"]:\n",
    "                rel_set.add(rel[\"predicate\"])\n",
    "               \n",
    "        # add tok span\n",
    "        \n",
    "        data = preprocessor.add_tok_span(data)\n",
    "\n",
    "        # check tok span\n",
    "        if config[\"check_tok_span\"]:\n",
    "            span_error_memory = check_tok_span(data)\n",
    "            if len(span_error_memory) > 0:\n",
    "                print(span_error_memory)\n",
    "            error_statistics[file_name][\"tok_span_error\"] = len(span_error_memory)\n",
    "            \n",
    "        file_name2data[file_name] = data\n",
    "pprint(error_statistics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output to Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:huayandan_val_0620 is output to ../data4bert/huayandan3/huayandan_val_0620.json\n",
      "INFO:root:huayandan_test_0620 is output to ../data4bert/huayandan3/huayandan_test_0620.json\n",
      "INFO:root:huayandan_train_0620 is output to ../data4bert/huayandan3/huayandan_train_0620.json\n",
      "INFO:root:rel2id is output to ../data4bert/huayandan3/rel2id.json\n",
      "INFO:root:ent2id is output to ../data4bert/huayandan3/ent2id.json\n",
      "INFO:root:data_statistics is output to ../data4bert/huayandan3/data_statistics.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'entity_type_num': 16,\n",
      " 'huayandan_test_0620': 55,\n",
      " 'huayandan_train_0620': 273,\n",
      " 'huayandan_val_0620': 69,\n",
      " 'relation_type_num': 9}\n"
     ]
    }
   ],
   "source": [
    "rel_set = sorted(rel_set)\n",
    "rel2id = {rel:ind for ind, rel in enumerate(rel_set)}\n",
    "\n",
    "ent_set = sorted(ent_set)\n",
    "ent2id = {ent:ind for ind, ent in enumerate(ent_set)}\n",
    "\n",
    "data_statistics = {\n",
    "    \"relation_type_num\": len(rel2id),\n",
    "    \"entity_type_num\": len(ent2id),\n",
    "}\n",
    "\n",
    "for file_name, data in file_name2data.items():\n",
    "    data_path = os.path.join(data_out_dir, \"{}.json\".format(file_name))\n",
    "    json.dump(data, open(data_path, \"w\", encoding = \"utf-8\"), ensure_ascii = False)\n",
    "    logging.info(\"{} is output to {}\".format(file_name, data_path))\n",
    "    data_statistics[file_name] = len(data)\n",
    "\n",
    "rel2id_path = os.path.join(data_out_dir, \"rel2id.json\")\n",
    "json.dump(rel2id, open(rel2id_path, \"w\", encoding = \"utf-8\"), ensure_ascii = False)\n",
    "logging.info(\"rel2id is output to {}\".format(rel2id_path))\n",
    "\n",
    "ent2id_path = os.path.join(data_out_dir, \"ent2id.json\")\n",
    "json.dump(ent2id, open(ent2id_path, \"w\", encoding = \"utf-8\"), ensure_ascii = False)\n",
    "logging.info(\"ent2id is output to {}\".format(ent2id_path))\n",
    "\n",
    "\n",
    "\n",
    "data_statistics_path = os.path.join(data_out_dir, \"data_statistics.txt\")\n",
    "json.dump(data_statistics, open(data_statistics_path, \"w\", encoding = \"utf-8\"), ensure_ascii = False, indent = 4)\n",
    "logging.info(\"data_statistics is output to {}\".format(data_statistics_path)) \n",
    "\n",
    "pprint(data_statistics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genrate WordDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if config[\"encoder\"] in {\"BiLSTM\", }:\n",
    "    all_data = []\n",
    "    for data in list(file_name2data.values()):\n",
    "        all_data.extend(data)\n",
    "        \n",
    "    token2num = {}\n",
    "    for sample in tqdm(all_data, desc = \"Tokenizing\"):\n",
    "        text = sample['text']\n",
    "        for tok in tokenize(text):\n",
    "            token2num[tok] = token2num.get(tok, 0) + 1\n",
    "    \n",
    "    token2num = dict(sorted(token2num.items(), key = lambda x: x[1], reverse = True))\n",
    "    max_token_num = 50000\n",
    "    token_set = set()\n",
    "    for tok, num in tqdm(token2num.items(), desc = \"Filter uncommon words\"):\n",
    "        if num < 3: # filter words with a frequency of less than 3\n",
    "            continue\n",
    "        token_set.add(tok)\n",
    "        if len(token_set) == max_token_num:\n",
    "            break\n",
    "        \n",
    "    token2idx = {tok:idx + 2 for idx, tok in enumerate(sorted(token_set))}\n",
    "    token2idx[\"<PAD>\"] = 0\n",
    "    token2idx[\"<UNK>\"] = 1\n",
    "#     idx2token = {idx:tok for tok, idx in token2idx.items()}\n",
    "    \n",
    "    dict_path = os.path.join(data_out_dir, \"token2idx.json\")\n",
    "    json.dump(token2idx, open(dict_path, \"w\", encoding = \"utf-8\"), ensure_ascii = False, indent = 4)\n",
    "    logging.info(\"token2idx is output to {}, total token num: {}\".format(dict_path, len(token2idx))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
