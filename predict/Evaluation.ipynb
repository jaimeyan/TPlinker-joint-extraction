{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'glove'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/09/06mjqyfn3lg67k5__mf3gnp00000gp/T/ipykernel_88126/355488931.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     24\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mwandb\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     25\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mconfig\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 26\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0mglove\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mGlove\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     27\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mnumpy\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'glove'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from IPython.core.debugger import set_trace\n",
    "from pprint import pprint\n",
    "import unicodedata\n",
    "from transformers import BertModel, BasicTokenizer, BertTokenizerFast\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "import glob\n",
    "import time\n",
    "from common.utils import Preprocessor\n",
    "from tplinker_plus import (HandshakingTaggingScheme,\n",
    "                           DataMaker4Bert,\n",
    "                           DataMaker4BiLSTM,\n",
    "                           TPLinkerPlusBert,\n",
    "                           TPLinkerPlusBiLSTM,\n",
    "                           MetricsCalculator)\n",
    "import wandb\n",
    "import config\n",
    "from glove import Glove\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = config.eval_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['24fad8c7']"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config['run_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(config[\"device_num\"])\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_home = config[\"data_home\"]\n",
    "experiment_name = config[\"exp_name\"]\n",
    "test_data_path = os.path.join(data_home, experiment_name, config[\"test_data\"])\n",
    "batch_size = config[\"hyper_parameters\"][\"batch_size\"]\n",
    "rel2id_path = os.path.join(data_home, experiment_name, config[\"rel2id\"])\n",
    "ent2id_path = os.path.join(data_home, experiment_name, config[\"ent2id\"])\n",
    "save_res_dir = os.path.join(config[\"save_res_dir\"], experiment_name)\n",
    "max_seq_len = config[\"hyper_parameters\"][\"max_seq_len\"]\n",
    "# for reproductivity\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_path_dict = {}\n",
    "for file_path in glob.glob(test_data_path):\n",
    "    file_name = re.search(\"(.*?)\\.json\", file_path.split(\"/\")[-1]).group(1)\n",
    "    test_data_path_dict[file_name] = file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'huayandan_test_0620': '../data4bert/huayandan1/huayandan_test_0620.json'}"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_path_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_dict = {}\n",
    "for file_name, path in test_data_path_dict.items():\n",
    "    test_data_dict[file_name] = json.load(open(path, \"r\", encoding=\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"encoder\"] == \"BERT\":\n",
    "    tokenizer = BertTokenizerFast.from_pretrained(config[\"bert_path\"], add_special_tokens=False, do_lower_case=False)\n",
    "    tokenize = tokenizer.tokenize\n",
    "    get_tok2char_span_map = lambda text:\n",
    "    tokenizer.encode_plus(text, return_offsets_mapping=True, add_special_tokens=False)[\"offset_mapping\"]\n",
    "elif config[\"encoder\"] in {\"BiLSTM\", }:\n",
    "    tokenize = lambda text: text.split(\" \")\n",
    "\n",
    "\n",
    "    def get_tok2char_span_map(text):\n",
    "        tokens = text.split(\" \")\n",
    "        tok2char_span = []\n",
    "        char_num = 0\n",
    "        for tok in tokens:\n",
    "            tok2char_span.append((char_num, char_num + len(tok)))\n",
    "            char_num += len(tok) + 1  # +1: whitespace\n",
    "        return tok2char_span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = Preprocessor(tokenize_func=tokenize,\n",
    "                            get_tok2char_span_map_func=get_tok2char_span_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculate the max token number: 100%|██████████| 98/98 [00:00<00:00, 475.48it/s]\n"
     ]
    }
   ],
   "source": [
    "all_data = []\n",
    "for data in list(test_data_dict.values()):\n",
    "    all_data.extend(data)\n",
    "\n",
    "max_tok_num = 0\n",
    "for sample in tqdm(all_data, desc=\"Calculate the max token number\"):\n",
    "    tokens = tokenize(sample[\"text\"])\n",
    "    max_tok_num = max(len(tokens), max_tok_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_tok_num: 1117, lagger than max_test_seq_len: 128, test data will be split!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Splitting into subtexts: 100%|██████████| 98/98 [00:00<00:00, 320.58it/s]\n"
     ]
    }
   ],
   "source": [
    "split_test_data = False\n",
    "if max_tok_num > max_seq_len:\n",
    "    split_test_data = True\n",
    "    print(\n",
    "        \"max_tok_num: {}, lagger than max_test_seq_len: {}, test data will be split!\".format(max_tok_num, max_seq_len))\n",
    "else:\n",
    "    print(\"max_tok_num: {}, less than or equal to max_test_seq_len: {}, no need to split!\".format(max_tok_num,\n",
    "                                                                                                  max_seq_len))\n",
    "max_seq_len = min(max_tok_num, max_seq_len)\n",
    "\n",
    "if config[\"hyper_parameters\"][\"force_split\"]:\n",
    "    split_test_data = True\n",
    "    print(\"force to split the test dataset!\")\n",
    "\n",
    "ori_test_data_dict = copy.deepcopy(test_data_dict)\n",
    "if split_test_data:\n",
    "    test_data_dict = {}\n",
    "    for file_name, data in ori_test_data_dict.items():\n",
    "        test_data_dict[file_name] = preprocessor.split_into_short_samples(data,\n",
    "                                                                          max_seq_len,\n",
    "                                                                          sliding_len=config[\"hyper_parameters\"][\n",
    "                                                                              \"sliding_len\"],\n",
    "                                                                          encoder=config[\"encoder\"],\n",
    "                                                                          data_type=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder(Tagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel2id = json.load(open(rel2id_path, \"r\", encoding=\"utf-8\"))\n",
    "ent2id = json.load(open(ent2id_path, \"r\", encoding=\"utf-8\"))\n",
    "handshaking_tagger = HandshakingTaggingScheme(rel2id, max_seq_len, ent2id)\n",
    "tag_size = handshaking_tagger.get_tag_size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"encoder\"] == \"BERT\":\n",
    "    tokenizer = BertTokenizerFast.from_pretrained(config[\"bert_path\"], add_special_tokens=False, do_lower_case=False)\n",
    "    data_maker = DataMaker4Bert(tokenizer, handshaking_tagger)\n",
    "    get_tok2char_span_map = lambda text:\n",
    "    tokenizer.encode_plus(text, return_offsets_mapping=True, add_special_tokens=False)[\"offset_mapping\"]\n",
    "\n",
    "elif config[\"encoder\"] in {\"BiLSTM\", }:\n",
    "    token2idx_path = os.path.join(*config[\"token2idx_path\"])\n",
    "    token2idx = json.load(open(token2idx_path, \"r\", encoding=\"utf-8\"))\n",
    "    idx2token = {idx: tok for tok, idx in token2idx.items()}\n",
    "\n",
    "\n",
    "    def text2indices(text, max_seq_len):\n",
    "        input_ids = []\n",
    "        tokens = text.split(\" \")\n",
    "        for tok in tokens:\n",
    "            if tok not in token2idx:\n",
    "                input_ids.append(token2idx['<UNK>'])\n",
    "            else:\n",
    "                input_ids.append(token2idx[tok])\n",
    "        if len(input_ids) < max_seq_len:\n",
    "            input_ids.extend([token2idx['<PAD>']] * (max_seq_len - len(input_ids)))\n",
    "        input_ids = torch.tensor(input_ids[:max_seq_len])\n",
    "        return input_ids\n",
    "\n",
    "\n",
    "    def get_tok2char_span_map(text):\n",
    "        tokens = text.split(\" \")\n",
    "        tok2char_span = []\n",
    "        char_num = 0\n",
    "        for tok in tokens:\n",
    "            tok2char_span.append((char_num, char_num + len(tok)))\n",
    "            char_num += len(tok) + 1  # +1: whitespace\n",
    "        return tok2char_span\n",
    "\n",
    "\n",
    "    data_maker = DataMaker4BiLSTM(text2indices, get_tok2char_span_map, handshaking_tagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../pretrained_models/BERT_EMR were not used when initializing BertModel: ['crf.transitions', 'hidden2label.bias', 'hidden2label.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "if config[\"encoder\"] == \"BERT\":\n",
    "    encoder = BertModel.from_pretrained(config[\"bert_path\"])\n",
    "    hidden_size = encoder.config.hidden_size\n",
    "    rel_extractor = TPLinkerPlusBert(encoder,\n",
    "                                     tag_size,\n",
    "                                     config[\"hyper_parameters\"][\"shaking_type\"],\n",
    "                                     config[\"hyper_parameters\"][\"inner_enc_type\"],\n",
    "                                     )\n",
    "\n",
    "elif config[\"encoder\"] in {\"BiLSTM\", }:\n",
    "    glove = Glove()\n",
    "    glove = glove.load(config[\"pretrained_word_embedding_path\"])\n",
    "\n",
    "    # prepare embedding matrix\n",
    "    word_embedding_init_matrix = np.random.normal(-1, 1, size=(len(token2idx), config[\"word_embedding_dim\"]))\n",
    "    count_in = 0\n",
    "\n",
    "    # 在预训练词向量中的用该预训练向量\n",
    "    # 不在预训练集里的用随机向量\n",
    "    for ind, tok in tqdm(idx2token.items(), desc=\"Embedding matrix initializing...\"):\n",
    "        if tok in glove.dictionary:\n",
    "            count_in += 1\n",
    "            word_embedding_init_matrix[ind] = glove.word_vectors[glove.dictionary[tok]]\n",
    "\n",
    "    print(\"{:.4f} tokens are in the pretrain word embedding matrix\".format(count_in / len(idx2token)))  # 命中预训练词向量的比例\n",
    "    word_embedding_init_matrix = torch.FloatTensor(word_embedding_init_matrix)\n",
    "\n",
    "    rel_extractor = TPLinkerPlusBiLSTM(word_embedding_init_matrix,\n",
    "                                       config[\"emb_dropout\"],\n",
    "                                       config[\"enc_hidden_size\"],\n",
    "                                       config[\"dec_hidden_size\"],\n",
    "                                       config[\"rnn_dropout\"],\n",
    "                                       tag_size,\n",
    "                                       config[\"hyper_parameters\"][\"shaking_type\"],\n",
    "                                       config[\"hyper_parameters\"][\"inner_enc_type\"],\n",
    "                                       )\n",
    "\n",
    "rel_extractor = rel_extractor.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = MetricsCalculator(handshaking_tagger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'24fad8c7'}\n",
      "model_state_dict_9.pt 24fad8c7\n",
      "model_state_dict_8.pt 24fad8c7\n",
      "model_state_dict_15.pt 24fad8c7\n",
      "model_state_dict_7.pt 24fad8c7\n",
      "model_state_dict_11.pt 24fad8c7\n",
      "model_state_dict_3.pt 24fad8c7\n",
      "model_state_dict_10.pt 24fad8c7\n",
      "model_state_dict_2.pt 24fad8c7\n",
      "model_state_dict_14.pt 24fad8c7\n",
      "model_state_dict_6.pt 24fad8c7\n",
      "model_state_dict_13.pt 24fad8c7\n",
      "model_state_dict_1.pt 24fad8c7\n",
      "model_state_dict_17.pt 24fad8c7\n",
      "model_state_dict_5.pt 24fad8c7\n",
      "model_state_dict_16.pt 24fad8c7\n",
      "model_state_dict_4.pt 24fad8c7\n",
      "model_state_dict_12.pt 24fad8c7\n",
      "model_state_dict_0.pt 24fad8c7\n"
     ]
    }
   ],
   "source": [
    "print(target_run_ids)\n",
    "for root, dirs, files in os.walk(model_state_dir):\n",
    "    for file_name in files:\n",
    "        run_id = root.replace('/files', '')[-8:]\n",
    "        if re.match(\".*model_state.*\\.pt\", file_name) and run_id in target_run_ids:\n",
    "            print(file_name, run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get model state paths\n",
    "model_state_dir = config[\"model_state_dict_dir\"]\n",
    "target_run_ids = set(config[\"run_ids\"])\n",
    "run_id2model_state_paths = {}\n",
    "for root, dirs, files in os.walk(model_state_dir):\n",
    "    for file_name in files:\n",
    "        run_id = root.replace('/files', '')[-8:]\n",
    "        if re.match(\".*model_state.*\\.pt\", file_name) and run_id in target_run_ids:\n",
    "            if run_id not in run_id2model_state_paths:\n",
    "                run_id2model_state_paths[run_id] = []\n",
    "            model_state_path = os.path.join(root, file_name)\n",
    "            run_id2model_state_paths[run_id].append(model_state_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'24fad8c7': ['./wandb/run-20220614_101807-24fad8c7/files/model_state_dict_9.pt',\n",
       "  './wandb/run-20220614_101807-24fad8c7/files/model_state_dict_8.pt',\n",
       "  './wandb/run-20220614_101807-24fad8c7/files/model_state_dict_15.pt',\n",
       "  './wandb/run-20220614_101807-24fad8c7/files/model_state_dict_7.pt',\n",
       "  './wandb/run-20220614_101807-24fad8c7/files/model_state_dict_11.pt',\n",
       "  './wandb/run-20220614_101807-24fad8c7/files/model_state_dict_3.pt',\n",
       "  './wandb/run-20220614_101807-24fad8c7/files/model_state_dict_10.pt',\n",
       "  './wandb/run-20220614_101807-24fad8c7/files/model_state_dict_2.pt',\n",
       "  './wandb/run-20220614_101807-24fad8c7/files/model_state_dict_14.pt',\n",
       "  './wandb/run-20220614_101807-24fad8c7/files/model_state_dict_6.pt',\n",
       "  './wandb/run-20220614_101807-24fad8c7/files/model_state_dict_13.pt',\n",
       "  './wandb/run-20220614_101807-24fad8c7/files/model_state_dict_1.pt',\n",
       "  './wandb/run-20220614_101807-24fad8c7/files/model_state_dict_17.pt',\n",
       "  './wandb/run-20220614_101807-24fad8c7/files/model_state_dict_5.pt',\n",
       "  './wandb/run-20220614_101807-24fad8c7/files/model_state_dict_16.pt',\n",
       "  './wandb/run-20220614_101807-24fad8c7/files/model_state_dict_4.pt',\n",
       "  './wandb/run-20220614_101807-24fad8c7/files/model_state_dict_12.pt',\n",
       "  './wandb/run-20220614_101807-24fad8c7/files/model_state_dict_0.pt']}"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_id2model_state_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_k_paths(path_list, k):\n",
    "    path_list = sorted(path_list, key=lambda x: int(re.search(\"(\\d+)\", x.split(\"/\")[-1]).group(1)))\n",
    "    #     pprint(path_list)\n",
    "    return path_list[-k:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only last k models\n",
    "k = config[\"last_k_model\"]\n",
    "for run_id, path_list in run_id2model_state_paths.items():\n",
    "    run_id2model_state_paths[run_id] = get_last_k_paths(path_list, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_duplicates(rel_list, ent_list):\n",
    "    rel_memory_set = set()\n",
    "    filtered_rel_list = []\n",
    "\n",
    "    for rel in rel_list:\n",
    "        rel_memory = \"{}\\u2E80{}\\u2E80{}\\u2E80{}\\u2E80{}\".format(rel[\"subj_tok_span\"][0],\n",
    "                                                                 rel[\"subj_tok_span\"][1],\n",
    "                                                                 rel[\"predicate\"],\n",
    "                                                                 rel[\"obj_tok_span\"][0],\n",
    "                                                                 rel[\"obj_tok_span\"][1])\n",
    "        if rel_memory not in rel_memory_set:\n",
    "            filtered_rel_list.append(rel)\n",
    "            rel_memory_set.add(rel_memory)\n",
    "\n",
    "    ent_memory_set = set()\n",
    "    filtered_ent_list = []\n",
    "    for ent in ent_list:\n",
    "        ent_memory = \"{}\\u2E80{}\\u2E80{}\".format(ent[\"tok_span\"][0],\n",
    "                                                 ent[\"tok_span\"][1],\n",
    "                                                 ent[\"type\"])\n",
    "        if ent_memory not in ent_memory_set:\n",
    "            filtered_ent_list.append(ent)\n",
    "            ent_memory_set.add(ent_memory)\n",
    "\n",
    "    return filtered_rel_list, filtered_ent_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(test_data, ori_test_data):\n",
    "    '''\n",
    "    test_data: if split, it would be samples with subtext\n",
    "    ori_test_data: the original data has not been split, used to get original text here\n",
    "    '''\n",
    "    indexed_test_data = data_maker.get_indexed_data(test_data, max_seq_len, data_type=\"test\")  # fill up to max_seq_len\n",
    "    test_dataloader = DataLoader(MyDataset(indexed_test_data),\n",
    "                                 batch_size=batch_size,\n",
    "                                 shuffle=False,\n",
    "                                 num_workers=6,\n",
    "                                 drop_last=False,\n",
    "                                 collate_fn=lambda data_batch: data_maker.generate_batch(data_batch, data_type=\"test\"),\n",
    "                                 )\n",
    "\n",
    "    pred_sample_list = []\n",
    "    for batch_test_data in tqdm(test_dataloader, desc=\"Predicting\"):\n",
    "        if config[\"encoder\"] == \"BERT\":\n",
    "            sample_list, batch_input_ids,\n",
    "            batch_attention_mask, batch_token_type_ids,\n",
    "            tok2char_span_list, _ = batch_test_data\n",
    "\n",
    "            batch_input_ids,\n",
    "            batch_attention_mask,\n",
    "            batch_token_type_ids = (batch_input_ids.to(device),\n",
    "                                    batch_attention_mask.to(device),\n",
    "                                    batch_token_type_ids.to(device),\n",
    "                                    )\n",
    "\n",
    "        elif config[\"encoder\"] in {\"BiLSTM\", }:\n",
    "            sample_list, batch_input_ids,\n",
    "            tok2char_span_list, _ = batch_test_data\n",
    "\n",
    "            batch_input_ids = batch_input_ids.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if config[\"encoder\"] == \"BERT\":\n",
    "                batch_pred_shaking_tag, _ = rel_extractor(batch_input_ids,\n",
    "                                                          batch_attention_mask,\n",
    "                                                          batch_token_type_ids,\n",
    "                                                          )\n",
    "            elif config[\"encoder\"] in {\"BiLSTM\", }:\n",
    "                batch_pred_shaking_tag, _ = rel_extractor(batch_input_ids)\n",
    "\n",
    "        batch_pred_shaking_tag = (batch_pred_shaking_tag > 0.).long()\n",
    "\n",
    "        for ind in range(len(sample_list)):\n",
    "            gold_sample = sample_list[ind]\n",
    "            text = gold_sample[\"text\"]\n",
    "            text_id = gold_sample[\"id\"]\n",
    "            tok2char_span = tok2char_span_list[ind]\n",
    "            pred_shaking_tag = batch_pred_shaking_tag[ind]\n",
    "            tok_offset, char_offset = 0, 0\n",
    "            if split_test_data:\n",
    "                tok_offset, char_offset = gold_sample[\"tok_offset\"], gold_sample[\"char_offset\"]\n",
    "            rel_list, ent_list = handshaking_tagger.decode_rel(text,\n",
    "                                                               pred_shaking_tag,\n",
    "                                                               tok2char_span,\n",
    "                                                               tok_offset=tok_offset, char_offset=char_offset)\n",
    "            pred_sample_list.append({\n",
    "                \"text\": text,\n",
    "                \"id\": text_id,\n",
    "                \"relation_list\": rel_list,\n",
    "                \"entity_list\": ent_list,\n",
    "            })\n",
    "\n",
    "    # merge\n",
    "    text_id2pred_res = {}\n",
    "    for sample in pred_sample_list:\n",
    "        text_id = sample[\"id\"]\n",
    "        if text_id not in text_id2pred_res:\n",
    "            text_id2pred_res[text_id] = {\n",
    "                \"rel_list\": sample[\"relation_list\"],\n",
    "                \"ent_list\": sample[\"entity_list\"],\n",
    "            }\n",
    "        else:\n",
    "            text_id2pred_res[text_id][\"rel_list\"].extend(sample[\"relation_list\"])\n",
    "            text_id2pred_res[text_id][\"ent_list\"].extend(sample[\"entity_list\"])\n",
    "\n",
    "    text_id2text = {sample[\"id\"]: sample[\"text\"] for sample in ori_test_data}\n",
    "    merged_pred_sample_list = []\n",
    "    for text_id, pred_res in text_id2pred_res.items():\n",
    "        filtered_rel_list, filtered_ent_list = filter_duplicates(pred_res[\"rel_list\"], pred_res[\"ent_list\"])\n",
    "        merged_pred_sample_list.append({\n",
    "            \"id\": text_id,\n",
    "            \"text\": text_id2text[text_id],\n",
    "            \"relation_list\": filtered_rel_list,\n",
    "            \"entity_list\": filtered_ent_list,\n",
    "        })\n",
    "\n",
    "    return merged_pred_sample_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_prf(pred_sample_list, gold_test_data, pattern=\"whole_text\"):\n",
    "    text_id2gold_n_pred = {}  # text id to gold and pred results\n",
    "\n",
    "    for sample in gold_test_data:\n",
    "        text_id = sample[\"id\"]\n",
    "        text_id2gold_n_pred[text_id] = {\n",
    "            \"gold_relation_list\": sample[\"relation_list\"],\n",
    "            \"gold_entity_list\": sample[\"entity_list\"],\n",
    "            \"gold_event_list\": sample[\"event_list\"],\n",
    "        }\n",
    "\n",
    "    for sample in pred_sample_list:\n",
    "        text_id = sample[\"id\"]\n",
    "        text_id2gold_n_pred[text_id][\"pred_relation_list\"] = sample[\"relation_list\"]\n",
    "        text_id2gold_n_pred[text_id][\"pred_entity_list\"] = sample[\"entity_list\"]\n",
    "\n",
    "    correct_num, pred_num, gold_num = 0, 0, 0\n",
    "    ent_correct_num, ent_pred_num, ent_gold_num = 0, 0, 0\n",
    "    ee_cpg_dict = {\n",
    "        \"trigger_iden_cpg\": [0, 0, 0],\n",
    "        \"trigger_class_cpg\": [0, 0, 0],\n",
    "        \"arg_iden_cpg\": [0, 0, 0],\n",
    "        \"arg_class_cpg\": [0, 0, 0],\n",
    "    }\n",
    "    ere_cpg_dict = {\n",
    "        \"rel_cpg\": [0, 0, 0],\n",
    "        \"ent_cpg\": [0, 0, 0],\n",
    "    }\n",
    "    for gold_n_pred in text_id2gold_n_pred.values():\n",
    "        gold_rel_list = gold_n_pred[\"gold_relation_list\"]\n",
    "        pred_rel_list = gold_n_pred[\"pred_relation_list\"] if \"pred_relation_list\" in gold_n_pred else []\n",
    "        gold_ent_list = gold_n_pred[\"gold_entity_list\"]\n",
    "        pred_ent_list = gold_n_pred[\"pred_entity_list\"] if \"pred_entity_list\" in gold_n_pred else []\n",
    "\n",
    "        if pattern == \"event_extraction\":\n",
    "            pred_event_list = handshaking_tagger.trans2ee(pred_rel_list, pred_ent_list)  # transform to event list\n",
    "            gold_event_list = gold_n_pred[\"gold_event_list\"]  # *\n",
    "            metrics.cal_event_cpg(pred_event_list, gold_event_list, ee_cpg_dict)\n",
    "        else:\n",
    "            metrics.cal_rel_cpg(pred_rel_list, pred_ent_list, gold_rel_list, gold_ent_list, ere_cpg_dict, pattern)\n",
    "\n",
    "    if pattern == \"event_extraction\":\n",
    "        trigger_iden_prf = metrics.get_prf_scores(ee_cpg_dict[\"trigger_iden_cpg\"][0],\n",
    "                                                  ee_cpg_dict[\"trigger_iden_cpg\"][1],\n",
    "                                                  ee_cpg_dict[\"trigger_iden_cpg\"][2])\n",
    "        trigger_class_prf = metrics.get_prf_scores(ee_cpg_dict[\"trigger_class_cpg\"][0],\n",
    "                                                   ee_cpg_dict[\"trigger_class_cpg\"][1],\n",
    "                                                   ee_cpg_dict[\"trigger_class_cpg\"][2])\n",
    "        arg_iden_prf = metrics.get_prf_scores(ee_cpg_dict[\"arg_iden_cpg\"][0], ee_cpg_dict[\"arg_iden_cpg\"][1],\n",
    "                                              ee_cpg_dict[\"arg_iden_cpg\"][2])\n",
    "        arg_class_prf = metrics.get_prf_scores(ee_cpg_dict[\"arg_class_cpg\"][0], ee_cpg_dict[\"arg_class_cpg\"][1],\n",
    "                                               ee_cpg_dict[\"arg_class_cpg\"][2])\n",
    "        prf_dict = {\n",
    "            \"trigger_iden_prf\": trigger_iden_prf,\n",
    "            \"trigger_class_prf\": trigger_class_prf,\n",
    "            \"arg_iden_prf\": arg_iden_prf,\n",
    "            \"arg_class_prf\": arg_class_prf,\n",
    "        }\n",
    "        return prf_dict\n",
    "    else:\n",
    "        rel_prf = metrics.get_prf_scores(ere_cpg_dict[\"rel_cpg\"][0], ere_cpg_dict[\"rel_cpg\"][1],\n",
    "                                         ere_cpg_dict[\"rel_cpg\"][2])\n",
    "        ent_prf = metrics.get_prf_scores(ere_cpg_dict[\"ent_cpg\"][0], ere_cpg_dict[\"ent_cpg\"][1],\n",
    "                                         ere_cpg_dict[\"ent_cpg\"][2])\n",
    "        prf_dict = {\n",
    "            \"rel_prf\": rel_prf,\n",
    "            \"ent_prf\": ent_prf,\n",
    "        }\n",
    "        return prf_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'24fad8c7': ['./wandb/run-20220614_101807-24fad8c7/files/model_state_dict_17.pt']}\n"
     ]
    }
   ],
   "source": [
    "for file_name, short_data in test_data_dict.items():\n",
    "    ori_test_data = ori_test_data_dict[file_name]\n",
    "    pprint(run_id2model_state_paths)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/09/06mjqyfn3lg67k5__mf3gnp00000gp/T/ipykernel_88126/2155932944.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     18\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     19\u001B[0m                 \u001B[0;31m# load model state\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 20\u001B[0;31m                 \u001B[0mrel_extractor\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload_state_dict\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel_state_path\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     21\u001B[0m                 \u001B[0mrel_extractor\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0meval\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     22\u001B[0m                 \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"run_id: {}, model state {} loaded\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrun_id\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmodel_state_path\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msplit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"/\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py\u001B[0m in \u001B[0;36mload\u001B[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001B[0m\n\u001B[1;32m    590\u001B[0m                     \u001B[0mopened_file\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mseek\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0morig_position\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    591\u001B[0m                     \u001B[0;32mreturn\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjit\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mopened_file\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 592\u001B[0;31m                 \u001B[0;32mreturn\u001B[0m \u001B[0m_load\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mopened_zipfile\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmap_location\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpickle_module\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mpickle_load_args\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    593\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0m_legacy_load\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mopened_file\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmap_location\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpickle_module\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mpickle_load_args\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    594\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py\u001B[0m in \u001B[0;36m_load\u001B[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001B[0m\n\u001B[1;32m    849\u001B[0m     \u001B[0munpickler\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpickle_module\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mUnpickler\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata_file\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mpickle_load_args\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    850\u001B[0m     \u001B[0munpickler\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpersistent_load\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpersistent_load\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 851\u001B[0;31m     \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0munpickler\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    852\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    853\u001B[0m     \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_utils\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_validate_loaded_sparse_tensors\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py\u001B[0m in \u001B[0;36mpersistent_load\u001B[0;34m(saved_id)\u001B[0m\n\u001B[1;32m    841\u001B[0m         \u001B[0mdata_type\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkey\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlocation\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msize\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdata\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    842\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mkey\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mloaded_storages\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 843\u001B[0;31m             \u001B[0mload_tensor\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata_type\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msize\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkey\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0m_maybe_decode_ascii\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlocation\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    844\u001B[0m         \u001B[0mstorage\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mloaded_storages\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mkey\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    845\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mstorage\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py\u001B[0m in \u001B[0;36mload_tensor\u001B[0;34m(data_type, size, key, location)\u001B[0m\n\u001B[1;32m    830\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    831\u001B[0m         \u001B[0mstorage\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mzip_file\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_storage_from_record\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msize\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstorage\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 832\u001B[0;31m         \u001B[0mloaded_storages\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mkey\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mrestore_location\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstorage\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlocation\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    833\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    834\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mpersistent_load\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msaved_id\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py\u001B[0m in \u001B[0;36mdefault_restore_location\u001B[0;34m(storage, location)\u001B[0m\n\u001B[1;32m    173\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0mdefault_restore_location\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstorage\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlocation\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    174\u001B[0m     \u001B[0;32mfor\u001B[0m \u001B[0m_\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0m_\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfn\u001B[0m \u001B[0;32min\u001B[0m \u001B[0m_package_registry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 175\u001B[0;31m         \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstorage\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlocation\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    176\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mresult\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    177\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mresult\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py\u001B[0m in \u001B[0;36m_cuda_deserialize\u001B[0;34m(obj, location)\u001B[0m\n\u001B[1;32m    149\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0m_cuda_deserialize\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mobj\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlocation\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    150\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mlocation\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstartswith\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'cuda'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 151\u001B[0;31m         \u001B[0mdevice\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mvalidate_cuda_device\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlocation\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    152\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mgetattr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mobj\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"_torch_load_uninitialized\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;32mFalse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    153\u001B[0m             \u001B[0mstorage_type\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mgetattr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcuda\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtype\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mobj\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__name__\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py\u001B[0m in \u001B[0;36mvalidate_cuda_device\u001B[0;34m(location)\u001B[0m\n\u001B[1;32m    133\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    134\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcuda\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mis_available\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 135\u001B[0;31m         raise RuntimeError('Attempting to deserialize object on a CUDA '\n\u001B[0m\u001B[1;32m    136\u001B[0m                            \u001B[0;34m'device but torch.cuda.is_available() is False. '\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    137\u001B[0m                            \u001B[0;34m'If you are running on a CPU-only machine, '\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU."
     ]
    }
   ],
   "source": [
    "# predict\n",
    "res_dict = {}\n",
    "predict_statistics = {}\n",
    "for file_name, short_data in test_data_dict.items():\n",
    "    ori_test_data = ori_test_data_dict[file_name]\n",
    "    for run_id, model_path_list in run_id2model_state_paths.items():\n",
    "        save_dir4run = os.path.join(save_res_dir, run_id)\n",
    "        if config[\"save_res\"] and not os.path.exists(save_dir4run):\n",
    "            os.makedirs(save_dir4run)\n",
    "\n",
    "        for model_state_path in model_path_list:\n",
    "            res_num = re.search(\"(\\d+)\", model_state_path.split(\"/\")[-1]).group(1)\n",
    "            save_path = os.path.join(save_dir4run, \"{}_res_{}.json\".format(file_name, res_num))\n",
    "\n",
    "            if os.path.exists(save_path):\n",
    "                pred_sample_list = [json.loads(line) for line in open(save_path, \"r\", encoding=\"utf-8\")]\n",
    "                print(\"{} already exists, load it directly!\".format(save_path))\n",
    "            else:\n",
    "                # load model state\n",
    "                rel_extractor.load_state_dict(torch.load(model_state_path))\n",
    "                rel_extractor.eval()\n",
    "                print(\"run_id: {}, model state {} loaded\".format(run_id, model_state_path.split(\"/\")[-1]))\n",
    "\n",
    "                # predict\n",
    "                pred_sample_list = predict(short_data, ori_test_data)\n",
    "\n",
    "            res_dict[save_path] = pred_sample_list\n",
    "            predict_statistics[save_path] = len([s for s in pred_sample_list if len(s[\"relation_list\"]) > 0])\n",
    "pprint(predict_statistics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "check char span: 100%|██████████| 824/824 [00:00<00:00, 209055.56it/s]\n"
     ]
    }
   ],
   "source": [
    "# check\n",
    "for path, res in res_dict.items():\n",
    "    for sample in tqdm(res, desc=\"check char span\"):\n",
    "        text = sample[\"text\"]\n",
    "        for rel in sample[\"relation_list\"]:\n",
    "            assert rel[\"subject\"] == text[rel[\"subj_char_span\"][0]:rel[\"subj_char_span\"][1]]\n",
    "            assert rel[\"object\"] == text[rel[\"obj_char_span\"][0]:rel[\"obj_char_span\"][1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save \n",
    "if config[\"save_res\"]:\n",
    "    for path, res in res_dict.items():\n",
    "        with open(path, \"w\", encoding=\"utf-8\") as file_out:\n",
    "            for sample in tqdm(res, desc=\"Output\"):\n",
    "                if len(sample[\"relation_list\"]) == 0:\n",
    "                    continue\n",
    "                json_line = json.dumps(sample, ensure_ascii=False)\n",
    "                file_out.write(\"{}\\n\".format(json_line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001B[0;32m/root/wycheng/research/tplinker/tplinker_plus/tplinker_plus.py\u001B[0m(262)\u001B[0;36mtrans2ee\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;32m    260 \u001B[0;31m            \u001B[0;32mif\u001B[0m \u001B[0mtirigger_offset2event\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mtrigger_offset_str\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m!=\u001B[0m \u001B[0mevent_type\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0;31m# filter false relations\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0m\u001B[0;32m    261 \u001B[0;31m                \u001B[0mset_trace\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0m\u001B[0;32m--> 262 \u001B[0;31m                \u001B[0;32mcontinue\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0m\u001B[0;32m    263 \u001B[0;31m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0m\u001B[0;32m    264 \u001B[0;31m            \u001B[0;31m# append arguments\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0m\n",
      "ipdb> event_type\n",
      "'Transport'\n",
      "ipdb> tirigger_offset2event[trigger_offset_str]\n",
      "'Attack'\n",
      "ipdb> tirigger_offset2score[trigger_offset_str]\n",
      "*** NameError: name 'tirigger_offset2score' is not defined\n",
      "ipdb> tirigger_offset2vote[trigger_offset_str]\n",
      "*** NameError: name 'tirigger_offset2vote' is not defined\n",
      "ipdb> trigger_offset2vote\n",
      "{'6,7': {'Attack': 4.1}, '11,12': {'Attack': 4.1, 'Transport': 1}}\n",
      "ipdb> rel_list\n",
      "[{'subject': 'marines', 'object': 'battled', 'subj_tok_span': [4, 6], 'obj_tok_span': [6, 7], 'subj_char_span': [5, 12], 'obj_char_span': [13, 20], 'predicate': 'Attacker_Attack'}, {'subject': 'snipers', 'object': 'battled', 'subj_tok_span': [7, 9], 'obj_tok_span': [6, 7], 'subj_char_span': [21, 28], 'obj_char_span': [13, 20], 'predicate': 'Attacker_Attack'}, {'subject': 'snipers', 'object': 'battled', 'subj_tok_span': [7, 9], 'obj_tok_span': [6, 7], 'subj_char_span': [21, 28], 'obj_char_span': [13, 20], 'predicate': 'Target_Attack'}, {'subject': 'snipers', 'object': 'fought', 'subj_tok_span': [7, 9], 'obj_tok_span': [11, 12], 'subj_char_span': [21, 28], 'obj_char_span': [37, 43], 'predicate': 'Attacker_Attack'}, {'subject': 'they', 'object': 'fought', 'subj_tok_span': [10, 11], 'obj_tok_span': [11, 12], 'subj_char_span': [32, 36], 'obj_char_span': [37, 43], 'predicate': 'Attacker_Attack'}, {'subject': 'capital', 'object': 'fought', 'subj_tok_span': [15, 16], 'obj_tok_span': [11, 12], 'subj_char_span': [60, 67], 'obj_char_span': [37, 43], 'predicate': 'Destination_Transport'}, {'subject': 'capital', 'object': 'fought', 'subj_tok_span': [15, 16], 'obj_tok_span': [11, 12], 'subj_char_span': [60, 67], 'obj_char_span': [37, 43], 'predicate': 'Place_Attack'}]\n",
      "ipdb> ent_list\n",
      "[{'type': 'Argument_Attacker', 'text': 'marines', 'tok_span': [4, 6], 'char_span': [5, 12]}, {'type': 'Argument_Target', 'text': 'marines', 'tok_span': [4, 6], 'char_span': [5, 12]}, {'type': 'Trigger_Attack', 'text': 'battled', 'tok_span': [6, 7], 'char_span': [13, 20]}, {'type': 'Argument_Attacker', 'text': 'snipers', 'tok_span': [7, 9], 'char_span': [21, 28]}, {'type': 'Argument_Target', 'text': 'snipers', 'tok_span': [7, 9], 'char_span': [21, 28]}, {'type': 'Argument_Artifact', 'text': 'they', 'tok_span': [10, 11], 'char_span': [32, 36]}, {'type': 'Argument_Attacker', 'text': 'they', 'tok_span': [10, 11], 'char_span': [32, 36]}, {'type': 'Trigger_Attack', 'text': 'fought', 'tok_span': [11, 12], 'char_span': [37, 43]}, {'type': 'Argument_Destination', 'text': 'capital', 'tok_span': [15, 16], 'char_span': [60, 67]}, {'type': 'Argument_Place', 'text': 'capital', 'tok_span': [15, 16], 'char_span': [60, 67]}]\n",
      "ipdb> q\n"
     ]
    },
    {
     "ename": "BdbQuit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mBdbQuit\u001B[0m                                   Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-25-86a01941a1df>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      5\u001B[0m         \u001B[0mfile_name\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mre\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msearch\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"(.*?)_res_\\d+\\.json\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfile_path\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msplit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"/\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgroup\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      6\u001B[0m         \u001B[0mgold_test_data\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mori_test_data_dict\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mfile_name\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 7\u001B[0;31m         \u001B[0mprf_dict\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mget_test_prf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpred_samples\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgold_test_data\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpattern\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mconfig\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"hyper_parameters\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"match_pattern\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      8\u001B[0m         \u001B[0mfilepath2scores\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mfile_path\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mprf_dict\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      9\u001B[0m     \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"---------------- Results -----------------------\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-21-1803cd982783>\u001B[0m in \u001B[0;36mget_test_prf\u001B[0;34m(pred_sample_list, gold_test_data, pattern)\u001B[0m\n\u001B[1;32m     34\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     35\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mpattern\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;34m\"event_extraction\"\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 36\u001B[0;31m             \u001B[0mpred_event_list\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mhandshaking_tagger\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrans2ee\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpred_rel_list\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpred_ent_list\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;31m# transform to event list\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     37\u001B[0m             \u001B[0mgold_event_list\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mgold_n_pred\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"gold_event_list\"\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;31m# *\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     38\u001B[0m             \u001B[0mmetrics\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcal_event_cpg\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpred_event_list\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgold_event_list\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mee_cpg_dict\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/root/wycheng/research/tplinker/tplinker_plus/tplinker_plus.py\u001B[0m in \u001B[0;36mtrans2ee\u001B[0;34m(self, rel_list, ent_list)\u001B[0m\n\u001B[1;32m    260\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mtirigger_offset2event\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mtrigger_offset_str\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m!=\u001B[0m \u001B[0mevent_type\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0;31m# filter false relations\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    261\u001B[0m                 \u001B[0mset_trace\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 262\u001B[0;31m                 \u001B[0;32mcontinue\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    263\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    264\u001B[0m             \u001B[0;31m# append arguments\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/root/wycheng/research/tplinker/tplinker_plus/tplinker_plus.py\u001B[0m in \u001B[0;36mtrans2ee\u001B[0;34m(self, rel_list, ent_list)\u001B[0m\n\u001B[1;32m    260\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mtirigger_offset2event\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mtrigger_offset_str\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m!=\u001B[0m \u001B[0mevent_type\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0;31m# filter false relations\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    261\u001B[0m                 \u001B[0mset_trace\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 262\u001B[0;31m                 \u001B[0;32mcontinue\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    263\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    264\u001B[0m             \u001B[0;31m# append arguments\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.8/bdb.py\u001B[0m in \u001B[0;36mtrace_dispatch\u001B[0;34m(self, frame, event, arg)\u001B[0m\n\u001B[1;32m     86\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0;31m# None\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     87\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mevent\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;34m'line'\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 88\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdispatch_line\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mframe\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     89\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mevent\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;34m'call'\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     90\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdispatch_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mframe\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0marg\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.8/bdb.py\u001B[0m in \u001B[0;36mdispatch_line\u001B[0;34m(self, frame)\u001B[0m\n\u001B[1;32m    111\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstop_here\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mframe\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbreak_here\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mframe\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    112\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0muser_line\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mframe\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 113\u001B[0;31m             \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mquitting\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0;32mraise\u001B[0m \u001B[0mBdbQuit\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    114\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrace_dispatch\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    115\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mBdbQuit\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# score\n",
    "if config[\"score\"]:\n",
    "    filepath2scores = {}\n",
    "    for file_path, pred_samples in res_dict.items():\n",
    "        file_name = re.search(\"(.*?)_res_\\d+\\.json\", file_path.split(\"/\")[-1]).group(1)\n",
    "        gold_test_data = ori_test_data_dict[file_name]\n",
    "        prf_dict = get_test_prf(pred_samples, gold_test_data, pattern=config[\"hyper_parameters\"][\"match_pattern\"])\n",
    "        filepath2scores[file_path] = prf_dict\n",
    "    print(\"---------------- Results -----------------------\")\n",
    "    pprint(filepath2scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}